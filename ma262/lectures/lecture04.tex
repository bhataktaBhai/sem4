\lecture{2024-01-16}{}
We get several results from this exercise.
The probability of ever hitting $-1$ is \begin{align*}
    1 - \Pr_0(T_{-1} = \infty) &= \lim_{s \uparrow 1} G(s) \\
        &= \frac{1 - \sqrt{1 - 4pq}}{2p} \\
        &= \frac{1 - \abs{2p - 1}}{2p} \\
        &= \begin{cases}
            1 & \text{if the walk is left-biased,} \\
            \frac{q}{p} & \text{otherwise.}
        \end{cases} \\
        &= \frac{q}{p} \wedge 1
\end{align*}
Another way to see the left-biased case is to note that $X_n$'s are sums of
iid $\mathrm{Ber}_{\pm}(p)$ random variables, and so by the strong law of
large numbers, \[
    \frac{X_n}{n} \asto p - q.
\] Thus if $p < q$, then $X_n \asto -\infty$.

\begin{exercise}
    Consider the SRW($p$) on $\Z$ started at the origin.
    Show that $T_{-2} \eqd \tau_1 + \tau_2$, where $\tau_1$ and $\tau_2$ are
    iid copies of $T_{-1}$.
\end{exercise}
\begin{solution}
    \begin{align*}
        \E_0[s^{T_{-2}}] &= \E_0[s^{T_{-2}} \1{T_{-1} < \infty}] \\
            &= \sum_m \E_0[s^{T_{-2}} \1{T_{-1} = m}] \\
            &= \sum_m \Pr_0(T_{-1} = m) \E_{-1}[s^{m + T_{-2}}] \\
            &= \sum_m \Pr_0(T_{-1} = m) s^m \E_{-1}[s^{T_{-2}}] \\
            &= \E_0[s^{T_{-1}}] \sum_m \Pr_0(T_{-1} = m) s^m \\
            &= G(s)^2
    \end{align*}
    On the other hand, $\E_0[s^{\tau_1 + \tau_2}] = \E_0[s^{\tau_1}]^2
    = G(s)^2$ by independence.

    Thus, $T_{-2} \eqd \tau_1 + \tau_2$ by \cref{thm:pgf}.
\end{solution}

\begin{exercise*}
    Consider the SRW($p$) on \Z{} started at the origin.
    Find $\Pr_0(T_{-1} = n)$, $n \in \N$.
\end{exercise*}
\begin{solution}
    We have $G(s) = \frac{1-\sqrt{1-4pqs^2}}{2ps}$.
    We first need a nice expression for $\binom{1/2}{k}$ in order to
    use the binomial theorem.
    \begin{align*}
        \binom{1/2}{k}
        &= \frac{\frac12(\frac12 - 1) \dots (\frac12 - k + 1)}{k!} \\
        &= \frac1{2^k} \frac{1(1 - 2) \dots (1 - 2k + 2)}{k!} \\
        &= \frac{(-1)^{k-1}}{2^k} \frac{(2k - 3)!!}{k!}
        \intertext{We can rewrite $(2k-3)!!$ as}
        (2k-3)!! &= \frac{(2k-3)!}{(2k-4)!!} \\
                 &= \frac{(2k-3)!}{2^{k-2} (k-2)!}
        \intertext{so that}
        \binom{1/2}{k}
        &= \frac{(-1)^{k-1}}{2^{2k-2}} \frac{(2k - 3)!}{k! (k-2)!} \\
        &= \frac{(-1)^{k-1}}{2^{2k-2} k} \binom{2k-3}{k-2}
        \intertext{but multiplying by $\frac{2k-2}{2(k-1)}$ yields an even
        nicer}
        &= \frac{(-1)^{k-1}}{2^{2k-1}} \frac{1}{k} \binom{2k-2}{k-1}
    \end{align*}
    The expression doesn't make sense for $k = 0$, for which the coefficient
    is $1$, and the derivation doesn't make sense for $k = 1$, but for which
    the expression happens to match the coefficient.

    But if we look closely, we see that this is just \[
        \frac{(-1)^{k-1}}{2^{2k-1}} C_{k-1}
    \] where $C_{k-1}$ is the $(k-1)$th
    \href{https://en.wikipedia.org/wiki/Catalan_number}{Catalan number}!

    From the binomial theorem, \begin{align*}
        (1 - x)^{\frac12} &= \sum_{k=0}^{\infty} \binom{1/2}{k} (-x)^k \\
            &= 1 - \sum_{k=1}^{\infty} \frac1{2^{2k-1}} C_{k-1} x^k
        \shortintertext{but more interestingly,}
        (1 - 4x)^{\frac12} &= 1 - 2 \sum_{k=1}^{\infty} C_{k-1} x^k
    \end{align*}
    In fact, this gives that \[
        \frac{1 - \sqrt{1 - 4x}}{2x} \text{ is the generating function for }
        (C_k)_{k \in \N}.
    \] Getting back to the problem at hand, we have \begin{align*}
        G(s) &= \frac{1 - \sqrt{1 - 4pqs^2}}{2ps} \\
            &= qs \sum_{k=0}^{n} C_k (pqs^2)^k
    \end{align*}
    So we have \[
        \Pr_0(T_{-1} = n) = \begin{cases}
            0 & \text{if $n$ is even} \\
            p^k q^{k+1} C_k & \text{if $n = 2k+1$}
        \end{cases}
    \] In the case of $p = q = \frac12$, this in fact proves that the
    number of SRW paths of length $2n$ from the origin to the origin, that
    never go below the origin, is the $n$th Catalan number.
    Why? Because each such path can be extended bijectively to a path of
    length $2n+1$ that hits $-1$ for the first time at time $2n+1$.
\end{solution}
% \begin{solution}[Alternate]
%     If one is allowed to use previous knowledge of the Catalan numbers,
%     then the following solution is much simpler.
%     \begin{align*}
%         \Pr(T_{-1} = 0) &= 0 \\
%         \Pr(T_{-1} = 1) &= q \\
%         \intertext{and for $n \ge 2$,}
%         \Pr(T_{-1} = n) &= p \Pr(T_{-2} = n-1) \\
%         &= p \sum_{k=1}^{n-2} \Pr(T_{-1} = k) \Pr(T_{-1} = n-1-k) \\
%     \end{align*}
%     Now let \[
%         a_n = \frac{\Pr(T_{-1} = n)}{p^{n-1} q^n}
%     \] so that \begin{align*}
%         a_0 &= 0 \\
%         a_1 &= 1 \\
%         a_n &= \sum_{k=1}^{n-2} 
%     \end{align*}
% \end{solution}
\section{Transience \& Recurrence} \label{sec:tnr}
\begin{definition*}
    Let $(X_n)_{n \in \N}$ be $MC_\mcS(\mu_0, P)$.
    Define $T_y = \text{\textcolor{Red}{???}} = \inf \set{n \in \N^* \mid X_n = y}$,
    where we take $\inf \O$ to be $+\infty$.

    For $x, y \in \mcS$, define $f_{xy} = \Pr_x(T_y < \infty)$.
    A state $x \in \mcS$ is said to be \emph{recurrent} if $f_{xx} = 1$, and
    \emph{transient} otherwise.

    A state $x$ is said to be \emph{absorbing} if $f_{xy} > 0$ only when
    $x = y$.
    (This is equivalent to $\set{x}$ being a communicating class.)

    We further define \begin{align*}
        N_y     &= \#\set{n \in \N^* \mid X_n = y}, \\
        G(x, y) &= \E_x[N_y].
    \end{align*}
    $G\colon \mcS^2 \to \R$ is called the \emph{Green's function}.
\end{definition*}

\begin{lemma} \label{thm:tnr:G}
    For all $x, y \in \mcS$, $G(x, y) = \sum_{n \in \N} p_{xy}^{(n)}$.
\end{lemma}
\begin{proof}
    We write $N_y$ as $\sum_{n=1}^{\infty} \1{X_n = y}$.
    Then, \begin{align*}
        G(x, y) &= \E_x\left[\sum_{n=1}^{\infty} \1{X_n = y}\right] \\
                &= \sum_{n=1}^{\infty} \E_x[\1{X_n = y}] \tag{MCT} \\
                &= \sum_{n=1}^{\infty} \Pr_x(X_n = y) \\
                &= \sum_{n=1}^{\infty} p_{xy}^{(n)}
    \end{align*}
    The interchange of the sum and the expectation is justified by the
    monotone convergence theorem stated below.
    % TODO: why is montone convergence theorem needed?
\end{proof}
\begin{fact*}[Monotone convergence theorem] \label{thm:tnr:mct}
    Let $(\Omega, \F, \Pr)$ be a probability space.
    Let $X_n\colon \Omega \to [0, \infty]$ be a sequence of random variables
    and $X\colon \Omega \to [0, \infty]$ be another random variable.
    Suppose that $X_n(\omega)\le X_{n+1}(\omega)$ for each $n$ and $\omega$,
    and that $X_n(\omega) \to X(\omega)$ for each $\omega$.
    Then, $\E[X_n] \to \E[X]$.
\end{fact*}
\begin{remark}
    The statement holds even if $X_n \asto X$.
\end{remark}
\begin{proof}
    We consider the case where $X_n\colon \Omega \to [0, \infty)$
    and $X\colon \Omega \to [0, \infty)$.

    Since $X_n \le X_{n+1} \le X$, we have \[
        \E[X_n] \le \E[X_{n+1}] \le \E[X].
    \] Thus the sequence $(\E[X_n])_{n \in \N}$ is increasing and bounded
    and so converges to some limit $L \le \E[X]$.
    \renewcommand{\qedsymbol}{\ensuremath{\dots}}
\end{proof}
What does this have to do with the Green's function?
\begin{align*}
    \E_x\left[\sum_{n=1}^{\infty} \1{X_n = y}\right]
        &= \E_x\left[\lim_{n \to \infty} S_n\right]
\end{align*}
where $S_n$'s are the partial sums.
Note that $S_n \le S_{n+1}$ and $S_n \to \sum_{n=1}^{\infty} p_{xy}^{(n)}$.
Thus applying the monotone convergence theorem, we get \begin{align*}
    \E_x\left[\sum_{n=1}^{\infty} \1{X_n = y}\right]
        &= \lim_{n \to \infty} \E_x[S_n] \\
        &= \lim_{n \to \infty} \sum_{m=1}^{n} \E_x[\1{X_m = y}] \\
        &= \sum_{n=1}^{\infty} \E_x[\1{X_n = y}].
\end{align*}

\begin{theorem*}
    For all $x, y \in \mcS$, \[
        \Pr_x(N_y = m) = \begin{cases}
            1 - f_{xy} & \text{if } m = 0 \\
            f_{xy} f_{yy}^{m-1} (1 - f_{yy}) & \text{if } m \in \N^* \\
            f_{xy} [f_{yy} = 1] & \text{if } m = +\infty
        \end{cases}
    \]
\end{theorem*}
\begin{proof}
    $N_y = 0$ if and only if $T_y = +\infty$.
    This occurs with probability $1 - f_{xy}$.

    We define $T_y^{(1)} = T_y$ and for $m \ge 1$, \[
        T_y^{(m+1)} = \inf\set{n > T_y^{(m)} \mid X_n = y}.
    \] Note that $T_y^{(m)} = +\infty$ implies $T_y^{(m+1)} = +\infty$.
Now \begin{align*}
        \Pr_x(T_y^{(m+1)} < \infty)
            &= \Pr_x(T_y^{(m)} < \infty \text{ and } T_y^{(m+1)} < \infty)\\
            &= \Pr_x(T_y^{(m)} < \infty) \Pr_y(T_y < \infty)
                \tag{Strong Markov property}
        \shortintertext{and by induction,}
        \Pr_x(N_y \ge m) &= \Pr_x(T_y^{(m)} < \infty) \\
                      &= f_{xy} f_{yy}^{m-1}. \tag{$*$}
                      \label{eq:tnr:prxny}
    \end{align*}
    The result follows by taking the difference.
    Or more directly, \[
        \Pr_x(N_y = m)
            = \Pr_x(T_y^{(m)} < \infty) \Pr_y(T_y = +\infty)
            = f_{xy} f_{yy}^{m-1} (1 - f_{yy}).
    \]
    Finally, \begin{align*}
        \Pr_x(N_y = +\infty)
            &= 1 - \sum_{m=0}^{\infty} \Pr_x(N_y = m) \\
            &= 1 - (1 - f_{xy}) - f_{xy} (1 - f_{yy})
                \sum_{m=0}^{\infty} f_{yy}^m \\
            &= \begin{cases}
                f_{xy} & \text{if } f_{yy} = 1 \\
                0 & \text{if } f_{yy} < 1
            \end{cases} \\
            &= f_{xy} [f_{yy} = 1]. \qedhere
    \end{align*}
\end{proof}

\begin{theorem*} \leavevmode
    \begin{enumerate}[label=(\arabic*)]
        \item Suppose $y$ is transient.
        Then for all $x \in \mcS$, $\Pr_x(N_y < \infty) = 1$ and
        $G(x, y) = \frac{f_{xy}}{1 - f_{yy}} < \infty$.
        \item Suppose $y$ is recurrent.
        Then $\Pr_y(N_y = \infty) = 1$ and $G(y, y) = +\infty$.
        Further, for all $x \in \mcS \setminus \set{y}$,
        $\Pr_x(N_y = \infty) = f_{xy}$ and \[
            G(x, y) = \begin{cases}
                0 & \text{if } f_{xy} = 0, \\
                \infty & \text{if } f_{xy} > 0.
            \end{cases}
        \]
    \end{enumerate}
\end{theorem*}
\begin{proof} \leavevmode
    \begin{enumerate}[label=(\arabic*)]
        \item Since $y$ is transient, $f_{yy} < 1$.
        Thus by the previous theorem, $\Pr_x(N_y = \infty) = 0$.
        Then, \begin{align*}
            G(x, y) &= \sum_{m=1}^{\infty} m \Pr_x(N_y = m) \\
                    &= f_{xy} (1 - f_{yy})
                        \sum_{m=1}^{\infty} m f_{yy}^{m-1} \\
                    &= f_{xy} (1 - f_{yy}) \frac{1}{(1 - f_{yy})^2} \\
                    &= \frac{f_{xy}}{1 - f_{yy}}.
        \end{align*}
        Alternatively, we use \cref{eq:tnr:prxny} to write \begin{align*}
            G(x, y) &= \sum_{m=1}^{\infty} \Pr_x(N_y \ge m) \\
                    &= \sum_{m=1}^{\infty} f_{xy} f_{yy}^{m-1} \\
                    &= \frac{f_{xy}}{1 - f_{yy}}.
        \end{align*}
        \item Since $y$ is recurrent, $f_{yy} = 1$.
        By the previous theorem, for any $x \in \mcS$, \[
            \Pr_x(N_y = m) = \begin{cases}
                1 - f_{xy} & \text{if } m = 0, \\
                0 & \text{if } m \in \N^*, \\
                f_{xy} & \text{if } m = +\infty.
            \end{cases}
        \] Thus $G(x, y) = +\infty$ if $f_{xy} > 0$ and $0$ otherwise.
        \qedhere
    \end{enumerate}
\end{proof}
\begin{corollary*}
    A state $x$ is recurrent iff
    $G(x, x) = \sum_{m=1}^{\infty} p_{xx}^{(m)} = +\infty$.
\end{corollary*}

\begin{definition*}
    A DTMC is said to be \emph{recurrent} (resp. \emph{transient}) if all
    its states are recurrent (resp. transient).
\end{definition*}

\begin{theorem*}
    If $\abs{\mcS} < \infty$, then there exists a recurrent state.
\end{theorem*}
\begin{proof}
    Suppose not.
    Then for all $x, y \in \mcS$, $G(x, y) = \sum_{m=0}^{\infty} p_{xy}^{(m)}
    < \infty$.
    Then the individual terms of the series must tend to $0$.
    Thus \begin{align*} % TODO: sum over x or y?
        \sum_{y \in \mcS} \lim_{m \to \infty} p_{xy}^{(m)}
            &= 0 \\
        \implies \lim_{m \to \infty} \sum_{y \in \mcS} p_{xy}^{(m)}
            &= 0.
    \end{align*}
    The interchange of the limit and the sum is justified since the sum
    is finite.
    But $\sum_{y \in \mcS} p_{xy}^{(m)} = 1$ for all $m$. % TODO: why?
    Thus we have a contradiction.
\end{proof}

\begin{theorem*} \label{thm:tnr:neighbour}
    Suppose $x \ne y \in \mcS$, $x$ is recurrent and $x \to y$.
    Then $y$ is recurrent, $y \to x$ and $f_{xy} = f_{yx} = 1$.
\end{theorem*}
\begin{proof}
    Since $x \to y$, there exists $n \in \N^*$ and $x_1,\dots,x_{n-1}$
    distinct from $x$ such that $p_{xx_1}p_{x_1x_2}\dots p_{x_{n-1}y} > 0$.
    Since $x$ is recurrent, \[
        0 = \Pr_x(T_x = +\infty)
            \ge p_{xx_1}p_{x_1x_2}\dots p_{x_{n-1}y} \Pr_y(T_x = +\infty)
    \] so $\Pr_y(T_x = +\infty)$ must be $0$.
    Thus $y \to x$ with $f_{yx} = 1$.
    If $y$ is recurrent, then $f_{xy}$ would be $1$ by the same argument.
    Thus we need only show that $y$ is recurrent.
    We can show this by showing that $G(y, y) = +\infty$.
    Let $p_{yx}^{(n_1)} > 0$ and $p_{xy}^{(n_2)} > 0$.
    \begin{align*}
        G(y, y) &\ge \smashoperator{\sum_{m=n_1+n_2+1}^{\infty}}
                    p_{yy}^{(m)} \\
                &\ge \sum_{r=1}^{\infty} p_{yx}^{(n_1)}
                    p_{xx}^{(r)} p_{xy}^{(n_2)} \\
                &= p_{yx}^{(n_1)} p_{xy}^{(n_2)} G(x, x) \\
                &= +\infty. \qedhere
    \end{align*}
\end{proof}
