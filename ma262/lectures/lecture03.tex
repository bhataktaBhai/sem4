\lecture{2024-01-11}{}
\begin{proof}[$\dots$]
    First consider the case $\abs{S} = 2$.
    Let $S = \set{s_1, s_2}$.
    We know that $g_S = a s_1 + b s_2$ for some $a, b \in \Z$.
    Now for any $m \in \N^*$, \begin{align*}
        m g_S &= ma s_1 + mb s_2 + ks_1s_2 - ks_1s_2 \\
            &= (ma - ks_2) s_1 + (mb + ks_1) s_2
    \end{align*} Choose $k \in \N$ such that $0 \le ma - ks_2 < s_2$.
    We can write $mg_S = a_m s_1 + b_m s_2$ where
    $0 \le a_m < s_2$.

    % TODO: why were the following three lines needed?
    Let $m_0$ be such that $m_0 g_S > s_1 s_2$.
    Then for all $m \ge m_0$, $mg_S - a_ms_1 > (s_2 - a) s_1 > 0$, so that
    $b_m > 0$.
    Thus $m_S = m_0$ works.

    Suppose the lemma holds for all sets of size $l-1$. \\
    Let $S = \set{s_1, s_2, \dots, s_l}$ and $F = S \setminus \set{s_l}$.
    Then by the previous lemma, $g_S = \gcd(g_F, s_l)$.

    Let $m_0$ be such that $m g_S - m_F g_F \ge m_{\set{g_F, s_l}}$.
    Then \begin{align*}
        m g_S - m_F g_F
            &= a g_F + b s_l \text{ for some } a \in \N, b \in \N^* \\
        m g_S &= (a + m_F) g_F + b s_l
        \shortintertext{but $a + m_F \ge m_F$, so we can write}
        m g_S &= \sum_{i=1}^{l-1} a_i s_i + b s_l
    \end{align*} where all $a_i$s are non-negative integers.
    This closes the induction.
\end{proof}
We can now prove \cref{thm:schur_recurrence}.
\begin{proof}
    Applying Schur's lemma to $F_x$, we have the existence of an $N$ such
    that $nd_x$ can be written as a non-negative integer combination of
    elements of $F_x$, for all $n \ge N$.
    Let $nd_x = \sum_{i=1}^{k} a_i f_i$.
    Then since $p_{ij}^{(m)} = (P^m)_{ij}$, \begin{align*}
        p_{xx}^{(nd_x)} &\ge \underbrace{p_{xx}^{(f_1)} \dots p_{xx}^{(f_1)}}_{a_1 \text{ times}} \dots \underbrace{p_{xx}^{(f_k)} \dots p_{xx}^{(f_k)}}_{a_k \text{ times}} \\
            &\ge \prod_{i=1}^{k} (p_{xx}^{(f_i)})^{a_i} \\
            &> 0 \qedhere
    \end{align*}
\end{proof}

\section{Stopping Times} \label{sec:stopping_times}
\begin{definition}[The Extended Reals] \label{def:extended_reals}
    The \emph{extended real line} is the set of real numbers along with $2$
    formal sumbols $+\infty$ and $-\infty$, denoted by \[
        \wbar{\R} = \R \cup \set{-\infty, +\infty}.
    \]
    $\wbar{\R}$ will be endowed with the order \[
        -\infty < x < \infty \text{ for all } x \in \R,
    \] along with the usual order on \R.
    We extend the algebraic operations on \R\ to $\wbar{\R}$.
    \begin{itemize}
        \item $x + \infty = +\infty$, $x - \infty = -\infty$ for all
            $x \in \R$.
        \item $x \cdot (+\infty) = +\infty$, $x \cdot (-\infty) = -\infty$
            for all $x \in \R$, $x > 0$.
        \item $x \cdot (+\infty) = -\infty$, $x \cdot (-\infty) = +\infty$
            for all $x \in \R$, $x < 0$.
        \item $\frac{x}{+\infty} = \frac{x}{-\infty} = 0$, for all
            $x \in \R$.
    \end{itemize}
    If $E \subseteq \R$ is not bounded above in \R, we say
    $\sup E = +\infty$.
    If $E = \O$, we say $\inf E = +\infty$.
\end{definition}

\begin{definition}[Filtration] \label{def:filtration}
    Let $(\Omega, \F, \Pr)$ be a probability space.
    A collection $(\F_n)_{n \in \N}$ of $\sigma$-algebras over $\Omega$ is
    called a \emph{filtration} if $\F_n \subseteq \F_{n+1} \subseteq \F$
    for all $n \in \N$.
\end{definition}
% Consider $X : (\Omega, \F, P) \to \overline{\R}$.
\begin{definition*}[Natural filtration] \label{def:natural_filtration}
    Let $(X_n)_{n \in \N}$ be a sequence of $\mcS$-valued random variables
    defined on $(\Omega, \F, \Pr)$.
    For $n \in \N$, define \begin{align*}
        \F_n &= \set{(X_0, X_1, \dots, X_n)^{-1}(A) \mid A \subseteq
                \mcS^{n+1}}
            &= \sigma(X_0, X_1, \dots, X_n)
    \end{align*}
    Here, $(X_0, \dots, X_n)^{-1}(A) = \set{\omega \in \Omega \mid
    (X_0(\omega), \dots, X_n(\omega)) \in A}$. \\
    This sequence of $\sigma$-algebras is called the \emph{natural
    filtration} of $(X_n)_{n \in \N}$.
\end{definition*}
\begin{remark}
    Note that elements of $\F_n$ are subsets of $\Omega$, not elements of
    $\Omega$.

    (GUESS) $\F_n$ is the set of all subsets of $\Omega$ subject to one
    condition: if $(X_0, \dots, X_n)(\omega_1)
    = (X_0, \dots, X_n)(\omega_2)$, then any set in $\F_n$ containing
    $\omega_1$ must also contain $\omega_2$.
    \begin{multline*}
        \F_n = \{A \subseteq \Omega \mid \forall \omega_1, \omega_2
            \in \Omega: \\
            (\omega_1 \in A) \lxor (\omega_2 \in A) \rightarrow
            (X_0, \dots, X_n)(\omega_1) \ne (X_0, \dots, X_n)(\omega_2)\}
    \end{multline*}
    % In fact, if one defines the equivalence relation $\sim_n$ on $\Omega$ by
    % $\omega \sim_n \omega'$ iff $X_i(\omega) = X_i(\omega')$ for each
    % $i \le n$, then $\F_n$ is simply $\Omega /{\sim_n}$.

    Why is $\F_n$ a $\sigma$-algebra?
    The empty set is in $\F_n$ because $\O \in \mcS^{n+1}$, and the set of
    pre-images of the null set is the null set.
    The complement of any set in $\F_n$ is in $\F_n$ because
    $(X_0, \dots, X_n)^{-1}(A)^c = (X_0, \dots, X_n)^{-1}(A^c)$.

    Why is $\F_n \subseteq \F_{n+1}$?
    For any $A \subseteq \mcS^{n+1}$, we have \[
        (X_0, \dots, X_n)^{-1}(A)
            = (X_0, \dots, X_n, X_{n+1})^{-1}(A \times \mcS).
    \]
\end{remark}

\begin{definition*}[Stopping time] \label{def:stopping_time}
    Suppose $(X_n)_{n \in \N}$ is a sequence of $\mcS$-valued random variables
    on $(\Omega, \F, (\F_n)_{n \in \N}, \Pr)$ where $(\F_n)_{n \in \N}$ is
    the natural filtration of $(X_n)_{n \in \N}$.

    Then $\tau\colon \Omega \to \N \cup \set{\infty}$ is called a
    \emph{stopping time} with respect to $(\F_n)_{n \in \N}$ if for all
    $n \in \N$, \[
        \set{\omega \in \Omega \mid \tau(\omega) \le n} \in \F_n.
    \]
\end{definition*}
This is equivalent to saying that for all $n \in \N$, \[
    \1{\set{\tau \le n}} = \1{\set{(X_0, \dots, X_n) \in A}}
    \quad \text{for some } A \in \mcS^{n+1}.
\]
(GUESS) In keeping with the previous guess, it is equivalent to the
following: if $\omega, \omega' \in \Omega$ with
$\tau(\omega) = n < \tau(\omega')$, then \[
    (X_0, \dots, X_n)(\omega) \ne (X_0, \dots, X_n)(\omega').
\]
Intuitively, a stopping time is a time at which we can decide whether or not
to stop the process based on the information available up to that time (in
measurable terms).

Consider the simple random walk $(X_n)_{n \in \N}$ on $\Z$.
Then the event that the hitting time of $10$ is at most $n$ is \[
    \set{T_{10} \le n} = \bigcup_{i=1}^{n} \set{X_i = 10}.
\]
\begin{examples}
    \item Let $(X_n)_{n \in \N}$ be an $\mcS$-valued stochastic process and let
    $A \subseteq \mcS$.
    Let $T_A \coloneq \inf\set{n \ge 1 \mid X_n \in A}$ (where we take
    $\inf \O$ to be $+\infty$).
    Then $T_A$ is a stopping time with respect to the natural filtration
    associated with $(X_n)_{n \in \N}$.
    That is, for all $n \in \N$, \[
        \set{T_A \le n} = \bigcup_{i=1}^{n} \set{X_i \in A} \in \F_n.
    \]
    Intuitively, say we stop as soon as we hit a desired state.
    Then we can decide whether or not to stop at time $n$ based on the
    information available up to time $n$.
    \item SRW($p$) started at the origin.
    Then $L = \sup \set{n \ge 1 \mid X_n < 7}$ is NOT a stopping time.
    Intuitively, $L$ is the \emph{last} time we are below $7$, which cannot
    be determined based on the information available from the past.
\end{examples}
\begin{proposition*} \label{thm:stopping_time}
    $\tau$ is a stopping time iff for all $n \in \N$, \[
        \set{\tau = n} \in \F_n.
    \]
\end{proposition*}
\begin{proof}
    Suppose $\tau$ is a stopping time.
    Then $\set{\tau = n} \in \F_n$ for all $n \in \N$.
    This is because \[
        \set{\tau = n} = \set{\tau \le n} \cap \set{\tau \le n-1}^c
    \] where both sets are in $\F_n \supseteq \F_{n-1}$,
    and therefore so is their intersection (de Morgan's law).

    Conversely, suppose $\set{\tau = n} \in \F_n$ for all $n \in \N$.
    Since $\F_0 \subseteq \F_1 \subseteq \dots \subseteq \F_n$, we have
    $\set{\tau = i} \in \F_n$ for all $i \le n$.
    Hence so is \[
        \set{\tau \le n} = \bigcup_{i=0}^{n} \set{\tau = i}. \qedhere
    \]
\end{proof}

\begin{proposition}
    If $\tau_1$ and $\tau_2$ are stopping times, then so are
    $\tau_1 \wedge \tau_2$, $\tau_1 \vee \tau_2$ and $\tau_1 + \tau_2$.
\end{proposition}
\begin{proof}
    We have \begin{align*}
        \set{\tau_1 \wedge \tau_2 \le n}
            &= \set{\tau_1 \le n} \cup \set{\tau_2 \le n} \\
        \set{\tau_1 \vee \tau_2 \le n}
            &= \set{\tau_1 \le n} \cap \set{\tau_2 \le n} \\
        \set{\tau_1 + \tau_2 \le n}
            &= \bigcup_{i=0}^{n} \set{\tau_1 \le i} \cap \set{\tau_2 \le n-i}
            \qedhere
    \end{align*}
\end{proof}
We can interpret the above operations as follows.
\begin{itemize}
    \item $\tau_1 \wedge \tau_2$ is the stopping time if we stop when either
    of our conditions are met.
    \item $\tau_1 \vee \tau_2$ is the stopping time if we stop when both of
    our conditions are met.
    \item $\tau_1 + \tau_2$ is the stopping time if we stop when we have
    waited for $\tau_1$ before we started looking for $\tau_2$.
\end{itemize}

\begin{exercise}
    Give an example of two stopping times $\tau_1$ and $\tau_2$ such that
    $\Pr(\tau_1 \le \tau_2) = 1$ but $\tau_2 - \tau_1$ is not a stopping
    time.
\end{exercise}
\begin{solution}
    Consider the SRW($p$) started at the origin, with \begin{align*}
        \tau_1 &= \inf\set{n \ge 1 \mid X_n = 1}, \\
        \tau_2 &= \inf\set{n \ge 1 \mid X_n = 2}. \qedhere
    \end{align*}
\end{solution}

\begin{theorem*}[Strong Markov property] \label{thm:smp}
    \,\\Let $(X_n)_{n \in \N}$ be $MC(\mu_0, P)$,
    and let $\tau$ be a stopping time.
    Let $A \subseteq \N$.
    Then \begin{multline*}
        \Pr_{\mu_0}(X_{\tau+1} = x_1, \dots, X_{\tau+n} = x_n
            \given \tau \in A, X_\tau = x) \\
        = \Pr_x(X_1 = x_1, \dots, X_n = x_n)
    \end{multline*}
\end{theorem*}
\begin{remark}
    The SMP is equivalent to \begin{equation*}
        \boxed{\E_{\mu_0}\left[f\big((X_{\tau+j})_{j \in \N}\big)
                        \mid \tau \in A, X_\tau = x\right]
            = \E_x\left[f\big((X_j)_{j \in \N}\big)\right]}
    \end{equation*} for any bounded function $f \colon \mcS^\infty \to \R$.
\end{remark}
\begin{proof}
    \begin{align*}
        \Pr_{\mu_0}(&X_{\tau+1} = x_1, \dots, X_{\tau+n} = x_n, \tau \in A,
        X_\tau = x) \\
            &= \sum_{m \in A} \Pr_{\mu_0}(X_{m+1} = x_1, \dots, X_{m+n} = x_n,
                \tau = m, X_m = x) \\
            &= \sum_{m \in A} \Pr_{\mu_0}(\tau = m, X_m = x) \\
            &\qquad\qquad \times \Pr_{\mu_0}(X_{m+1} = x_1, \dots,
                X_{m+n} = x_n \given \tau = m, X_m = x) \\
            &= \sum_{m \in A} \Pr_{\mu_0}(\tau = m, X_m = x)
                \Pr_{x}(X_1 = x_1, \dots, X_n = x_n)
        \intertext{by the \nameref{thm:mp}, since $\tau = m$ is equivalent
        to $(X_0, \dots, X_m)$ belonging to some set in $\F_m$.
        Summing over $A$ gives}
            &= \Pr_{\mu_0}(\tau \in A, X_\tau = x)
                \Pr_{x}(X_1 = x_1, \dots, X_n = x_n)
    \end{align*}
    and dividing by $\Pr_{\mu_0}(\tau \in A, X_\tau = x)$ yields the result.
\end{proof}

\begin{definition*} \label{def:pgf}
    Suppose $X$ takes values in $\N \cup \set{\infty}$, and let
    $p_k \coloneq \Pr(X = k)$, $k \in \N \cup \set{\infty}$.
    Then the \emph{probability generating function} of $X$ is defined as
    \begin{align*}
        G_X(s) &= p_0 + p_1 s + p_2 s^2 + \dots, \quad s \in (-1, 1) \\
            &= \E[s^X]
    \end{align*} where we take $s^\infty = 0$ for $\abs{s} < 1$.
\end{definition*}
\begin{remark}
    The left limit $G_X(1^-) = \lim_{s \uparrow 1} G_X(s) = 1 - p_\infty$.
\end{remark}
If $p_\infty > 0$, then $\E[X] = \infty$.
Otherwise, \[
    \E[X] = \sum_{k=0}^{\infty} k p_k
          = \sum_{k=0}^{\infty} p_k k (1)^{k-1}
          = G_X'(1^-)
\]

\begin{theorem*} \label{thm:pgf}
    Let $X$, $Y$ be two random variables taking values in
    $\N \cup \set{\infty}$ and $G_X(s) = G_Y(s) \;\forall s \in (-1, 1)$.
    Then $X \eqd Y$.
\end{theorem*}
\begin{proof}
    Since $G_X, G_Y \in C^\infty(-1, 1)$, we have \[
        G_X^{(n)} = G_Y^{(n)} \text{ for all } n \in \N.
    \] But $G_X^{(n)}(0) = n! p_n^X$.
    Thus $p_n^X = p_n^Y$ for all $n \in \N$.
    % TODO: where is rigour?
\end{proof}

\begin{exercise*}
    Suppose $(X_n)_{n \in \N}$ is an SRW($p$) on \Z\ started at the origin.
    Find $G_{T_{-1}}$,
    where $T_{-1}$ is the first hitting time of $-1$.
\end{exercise*}
\begin{solution}
    Denote $G_{T_{-1}}$ by $G$ for simplicity.
    \begin{align*}
        G(s) &= \E_0[s^{T_{-1}}] \\
            &= p \E_0[s^{T_{-1}} \given X_1 = 1]
                + q \E_0[s^{T_{-1}} \given X_1 = -1] \\
            &= p \E_1[s^{1 + T_{-1}}] + q s \\
            &= p s \E_1[s^{T_{-1}}] + q s \\
            &= p s \E_0[s^{T_{-2}}] + q s
    \end{align*}
    Since $s^\infty = 0$ by our convention, we have \begin{align*}
        \E_0[s^{T_{-2}}] &= \E_0[s^{T_{-2}} \1{T_{-1} < \infty}] \\
            &= \sum_{m} \E_0[s^{T_{-2}} \1{T_{-1} = m}] \\
            &= \sum_{m} \Pr_0(T_{-1} = m) \E_{-1}[s^{m + T_{-2}}] \\
            &= \sum_{m} \Pr_0(T_{-1} = m) s^m \E_{-1}[s^{T_{-2}}] \\
            &= \E_0[s^{T_{-1}}] \sum_{m} \Pr_0(T_{-1} = m) s^m \\
            &= G(s)^2
    \end{align*}
    Thus \begin{align*}
        G(s) &= p s G(s)^2 + q s \\
        G(s) &= \frac{1 \pm \sqrt{1 - 4pq s^2}}{2ps}
    \end{align*}
    \textbf{Claim:} $G(s) = \frac{1 - \sqrt{1 - 4pqs^2}}{2ps}$ for all
    $s \in (-1, 1) \setminus \set{0}$.
    % This is becuas the $+$ solution does not converge at $0$.
    % TODO: WTF how
\end{solution}
