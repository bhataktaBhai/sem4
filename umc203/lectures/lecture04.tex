\lecture{04}{Thu 18 Jan '24}{}
Suppose we also have a loss function $\ell\colon \mathcal{Y} \times
\mathcal{Y} \to \R_{\ge 0}$.
Then we have the \emph{Bayes error rate} \[
    E_{Y \given X = x} \ell(\tilde{h}(x), Y)
        = \min_{i \in [M]} r_i(x)
\] where $r_i(x) = E_{Y \given X = x} \ell(i, Y)$.

We define the \emph{Bayes risk} \[
    R(h) = E_{XY} \ell(h(X), Y)
\]
\begin{theorem}
    Given the loss function $\ell = 1 - \delta$, where $\delta$ is the
    Kronecker delta, the Bayes classifier minimises the Bayes risk.
    That is, $h$
\end{theorem}
\begin{proof}
    We have \begin{align*}
        r_i(x) &= \sum_{j = 1}^{M} \ell(i, j) \eta_j(x) \\
            &= \sum_{k \ne i} \eta_k(x) \\
            &= 1 - \eta_i(x)
    \end{align*}
    We choose $\tilde{h}(x) = \argmin_i r_i(x)$.
    Then for all $j \ne i$, \begin{align*}
        r_i(x) &< r_j(x) \\
        1 - \eta_i(x) &< 1 - \eta_j(x) \\
        \eta_j(x) &< \eta_i(x)
    \end{align*}
    For all $j \ne i$, $\tilde{h} = h^*$.
\end{proof}

For $M$ category problem, define discriminant functions
$g_i\colon \mathcal{X} \to \R$ for $i \in [M]$, and define the classifier
$h(x) = \argmax_i g_i(x)$.

Let $f\colon [0, 1] \to \R$ be any monotonically increasing function.
Then $g_i = f \circ \eta_i$ works as a discriminant.

Suppose $\Pr(X = x \given Y = i) = N(x \given \mu_i, C_i)$.
Then \[
    \eta_i(x) = \frac{p_i N(x \given \mu_i, C_i)}{P(x)}.
\]
Then \begin{align*}
    \log \eta_i(x) &= \log p_i + \log N(x \given \mu_i, C_i) - \log P(x) \\
          &= \log p_i + \log \frac1{\big(\sqrt{2\pi}\big)^d \abs{C}^{1 / 2}}
          - \frac12 (x - \mu_i)^\top C^{-1} (x - \mu_i)
          - \log P(x)
\end{align*}
If $C_i = C$ for all $i$, we drop the constants to get \[
    g_i(x) = \log p_i - \frac12 (x - \mu_i)^\top C^{-1} (x - \mu_i).
\]
