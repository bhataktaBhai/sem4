\section{Risk Analysis} \label{sec:perceptron:risk}
\lecture{2024-02-01}{Generalization error of the perceptron algorithm}
Let $\mcD$ be a training set of size $N$ drawn i.i.d. from $P$.
We denote this as $\mcD \sim P^N$.

That is, \begin{align*}
    \mcD &= \set{(x^{(i)}, y_i) \mid x^{(i)} \in \mcX, y_i \in \mcY,
                                i \in [N]} \\
    \mcD &\sim P^{(N)} \\
    \mcX &\subseteq \R^d \\
    \mcY &= \pms.
\end{align*}

Suppose there exists a $w^* \in \R^d$ such that for each $i \in [N]$,
$\sgn \innerp{w^*}{x^{(i)}} = y_i$. \\
Then the algorithm described in the previous lecture will find a $w$ that
separates the samples in at most $\frac{R^2}{\gamma^{*2}}$ iterations,
where $R$ is the maximum norm of $x^{(i)}$s (\emph{radius}) and \[
    \gamma^* = \gamma(w^*)
        = \min_{i \in [N]} \frac{\abs*{w^{*\top} x^{(i)}}}{\norm{w^*}}
\]

Let \mcD\ be linearly separable and let the Perceptron algorithm return a
classifier $h_\mcD^{(p)}$.
We have risk $R(h_\mcD^{(p)}) = \Pr_{X, Y \sim P}(h_\mcD^{(p)}(X) \neq Y)$
% (\textcolor{% exercise}{under what distribution?}).
We compute the \emph{expected generalization error} by a classifier returned
by the Perceptron algorithm acting on a linearly separable sample of size
$N$ drawn iid from $P$.
That is, we compute \[
    \E_{\mcD \sim P^{N}}[R(h_\mcD^{(p)})].
\] This is hard!

We will instead compute the proxy $\wbar{R}^{LOO}_\mcD(A)$, where $A$ is an
algorithm acting on a sample \mcD\ of size $m$, returning a classifier
$h_\mcD^{A}$.

\subsection{Leave-One-Out Error} \label{sec:loo}

\begin{definition*}[Leave-one-out error] \label{def:loo}
    Let $A$ be an algorithm that acts on a sample
    $\mcD = \set{(x^{(i)}, y_i) \mid i \in [m]}$ to return a
    classifier $h_\mcD^{A}$.
    The \emph{leave-one-out error} of $A$ on $\mcD$ is defined to be \[
        \wbar{R}^{LOO}_\mcD(A) \coloneq \frac1m \sum_{i=1}^{m}
            \left[h_{\mcD_{(i)}}^A\!\!\left(x^{(i)}\right) \ne y_i\right],
    \] where $\mcD_{(i)} = \mcD \setminus \set{(x^{(i)}, y_i)}$.
\end{definition*}

We want to compute the expected value of $\wbar{R}^{LOO}_\mcD(A)$ over
all samples \mcD\ of size $m$ drawn iid from $P$.
\begin{align*}
    \E_{\mcD \sim P^m}[\wbar R^{\text{LOO}}_\mcD(A)]
    &= \frac1m \E_{\mcD \sim P^m} \sum_{i=1}^{m}
        \left[h_{\mcD_{(i)}}^A\!\!\left(x^{(i)}\right) \ne y_i\right]
    \intertext{Since the samples are iid, we have}
    \E_{\mcD \sim P^m}
        &= \E_{\mcD \sim P^{m-1}} \E_{(x^{(i)}, y_i) \sim P}
    \intertext{We first compute the inner expectation.}
    \E_{(x^{(i)}, y_i) \sim P} \left[h_{\mcD_{(i)}}^A\!\!
    \left(x^{(i)}\right) \ne y_i \right]
        &= \Pr \left(h^A_{\mcD_{(i)}}\!\!\left(x^{(i)}\right)
                \ne y_i\right) \\
        &= R(h^A_{\mcD_{(i)}})
    \intertext{So we have}
    \E_{\mcD \sim P^m}[\wbar R^{\text{LOO}}_\mcD(A)]
    &= \frac1m \sum_{i=1}^{m} \E_{\mcD_{(i)} \sim P^{m-1}}
        R(h^A_{\mcD_{(i)}}) \\
    &= \E_{\mcD \sim P^{m-1}} R(h^A_{\mcD}).
\end{align*}
Thus the expected leave-one-out error of an algorithm $A$ acting on a sample
of size $m$ drawn iid from $P$ is the expected risk of the classifier
returned by $A$ acting on a sample of size $m-1$ drawn iid from $P$.
Thus we have proven the following theorem.
\begin{theorem*} \label{thm:loo}
    Let $A$ be an algorithm that acts on a sample \mcD{} to return a
    classifier $h_\mcD^{A}$.
    Then the expected leave-one-out error of $A$ acting on a sample of size
    $m$ drawn iid from a probability distribution $P$
    is the expected risk of the classifier returned by $A$
    acting on a sample of size $m-1$ drawn iid from $P$.
    \[
        \E_{\mcD \sim P^m}[\wbar R^{\text{LOO}}_\mcD(A)]
        = \smashoperator{\E_{\mcD \sim P^{m-1}}} [R(h^A_{\mcD})].
    \]
\end{theorem*}

\subsection{Perceptron's Leave-One-Out Error} \label{sec:perceptron:loo}
Fix a sample $\mcD \sim P^{N+1}$.
Let $i_k$ be the index of the sample misclassified in the $k$th iteration.
Then the $w$ returned by the Perceptron algorithm is \[
    w^{(k)} = \sum_{j=1}^{M} y_{i_j} x^{(i_j)}
\] where $M = M(\mcD)$ is the number of iterations.
Let $I = \set{i_1, i_2, \ldots, i_M}$.
Then for any index $i \notin I$,
$w$ correctly classifies $x^{(i)}$ at every iteration.
Thus it makes no difference to the algorithm whether $x^{(i)}$ is in the
sample or not.

The classifiers $h_{\mcD_{(i)}}^{(p)}$ and $h_{\mcD}^{(p)}$ are the same,
and so the leave-one-out error \[
    [h_{\mcD_{(i)}}^{(p)}(x^{(i)}) \ne y_i]
\] for this $i$ is $0$.

Thus the average leave-one-out error on \mcD\ is at most \[
    \frac1{N+1} \sum_{i \in U} 1 = \frac{M(\mcD)}{N+1}.
\] By the previous bound on the number of iterations, we have
\begin{theorem*} \label{thm:perceptron:error}
    The expected generalization error of the perceptron algorithm \[
        \E_{\mcD \sim P} R(h_\mcD^{(p)})
            \le \frac{M(D)}{N + 1}
            \le \frac{\rho(\mcD)^2}{(N+1) \gamma^{*}(\mcD)^2}
    \] where $M(\mcD)$ is the number of iterations that the perceptron
    algorithm takes to converge on \mcD,
    and $\rho(\mcD)$ and $\gamma^{*}(\mcD)$ are the radius and margin
    of \mcD{} respectively.
\end{theorem*}
