\lecture{02}{Thu 11 Jan '24}{}

We come back to apples and oranges.
\begin{align*}
    \mathcal{X} \subseteq \R^d \\
    \mathcal{Y} &= \set{-1, 1} \\
    \mathcal{D} &= \set{(x_i, y_i) \mid x_i \in \mathcal{X},
        y_i \in \mathcal{Y}}
\end{align*}
We also know \emph{priors} \begin{align*}
    \Pr(Y = 1) &= p_1 & \Pr(Y = -1) = p_2 &= 1 - p_1 \\
    \intertext{and \emph{class condition distributions}}
    \Pr(X = x \mid Y = 1) &= f_1(x) & \Pr(X = x \mid Y = -1) &= f_2(x)
\end{align*}
\begin{remark}
    We will always write probabilities like this, but understand them to be
    densities whenever appropriate.
\end{remark}
From Bayes' rule, we have
\begin{align*}
    \eta(x) \coloneq \Pr(Y = 1 \mid X = x)
        &= \frac{\Pr(X = x \mid Y = 1) \Pr(Y = 1)}{\Pr(X = x)} \\
        &= \frac{f_1(x) p_1}{f_1(x) p_1 + f_2(x) p_2}
\end{align*}
We can then define the \emph{Bayes classifier} as \begin{align*}
    h^*(x) &\coloneq \sgn(2\eta(x) - 1) \\
    &= \begin{cases}
        \hphantom{-}1 & \text{if } \eta(x) > \frac{1}{2}, \\
        -1 & \text{otherwise}
    \end{cases} \\
    &= \begin{cases}
        \hphantom{-}1 & \text{if } f_1(x) p_1 > f_2(x) p_2, \\
        -1 & \text{otherwise}
    \end{cases}
\end{align*}

For the specific case of multivariate Gaussians, \ie, $f_1$ and $f_2$ of the
form \begin{align*}
    N(x \mid \mu, C) &= \frac{1}{\sqrt{(2\pi)^d \det(C)}}
        \exp\left(-\frac{1}{2}(x - \mu)^\top C^{-1} (x - \mu)\right),
\end{align*} with same covariance $C$ but different means $\mu_1$ and $\mu_2$,
we write \[
    h^*(x) = \begin{cases}
        \hphantom{-}1 & \text{if } \log \frac{\eta(x)}{1 - \eta(x)} > 0 \\
        -1 & \text{otherwise}
    \end{cases}
\] Now \begin{align*}
    \log \frac{\eta(x)}{1 - \eta(x)}
        &= \log \frac{f_1(x) p_1}{f_2(x) p_2} \\
        &= \log \frac{p_1}{p_2} - \frac12 \left((x-\mu_1)^\top C^{-1}(x-\mu_1)\right) \\
        &\qquad + \frac12 \left((x-\mu_2)^\top C^{-1}(x-\mu_2)\right) \\
        &= \log \frac{p_1}{p_2} + (\mu_1 - \mu_2)^\top C^{-1} x
            - \frac12 (\mu_1^\top C^{-1} \mu_1 - \mu_2^\top C^{-1} \mu_2) \\
        &= w^\top x - b
\end{align*} where $w = C^{-1}(\mu_1 - \mu_2)$ and $b$ is something.
Thus $h^*(x) = \sgn(w^\top x - b)$.
\begin{remark}
    $w^\top x = b$ is a hyperplane in $\R^d$, dividing the space into two
    half-spaces: $w^\top x < b$ and $w^\top x > b$.
    So a line is a very good guess for a classifier!
\end{remark}

\begin{exercise}
    Examine the special case of $C_1 = \sigma_1^2 I$ and
    $C_2 = \sigma_2^2 I$.
\end{exercise}
\begin{solution}
    We have \begin{equation*}
        \log \frac{f_1(x)}{f_2(x)}
            = \log \frac{\sigma_2^d}{\sigma_1^d}
                - \frac{1}{2\sigma_1^2} \norm{x - \mu_1}^2
                + \frac{1}{2\sigma_2^2} \norm{x - \mu_2}^2
    \end{equation*}
\end{solution}

\subsection{How Good is the Bayes Classifier?} \label{sec:goodness}
We wish to compute the error $\Pr(h(X) \ne Y)$ for some rule $h$.
\begin{align*}
    \Pr(h(X) \ne Y)
        &= E_{XY} \ind{h^*(X) \ne Y} \\
        &= E_X E_{Y \mid X} \ind{h^*(X) \ne Y} \\
    \intertext{but}
    E_{Y \mid X} \ind{h^*(X) \ne Y}
        &= \eta(x) \ind{h^*(x) \ne 1} + (1 - \eta(x)) \ind{h^*(x) \ne -1}
\end{align*}
Comparing it with $h^*$,
\begin{align*}
    E_{Y \mid X} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y})
        &= E_{Y \given X} (\ind{h^*(X) = Y} - \ind{h^*(X) \ne Y}) \\
        &= \eta(x) (\ind{h^*(X) = 1} - \ind{h^*(X) = -1}) \\
        &\qquad + (1 - \eta(x)) (\ind{h^*(X) = -1} - \ind{h^*(X) = 1}) \\
        &= (2 \eta(x) - 1)(\ind{h^*(X) = 1} - \ind{h^*(X) = -1}) \\
        &= (2 \eta(x) - 1)(2 \ind{h^*(X) = 1} - 1)
\end{align*}
The second term is $1$ when the first term is positive, and $-1$ when it is
negative.

Thus \begin{align*}
    E_{XY} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y})
        &= E_X E_{Y \mid X} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y}) \\
        &= E_X \abs{2 \eta(x) - 1} \ge 0.
\end{align*}
This proves that the Bayes classifier is the classifier with the lowest
probability of error. \\
(This is theorem 2.1 in DGL.)
