\lecture{2024-01-11}{Bayes classifier, multivariate gaussians and optimality}

We come back to apples and oranges.
\begin{align*}
    \mathcal{X} &\subseteq \R^d \quad \text{instance space} \\
    \mathcal{Y} &= \set{-1, 1} \quad \text{label space} \\
    \mathcal{D} &= \set{(x_i, y_i) \mid x_i \in \mathcal{X},
        y_i \in \mathcal{Y}}
\end{align*}
We also know \emph{priors} \begin{align*}
    \Pr(Y = 1) &= p_1 & \Pr(Y = -1) &= 1 - p_1 \eqcolon p_2
    \intertext{and \emph{class conditioned distributions}}
    \Pr(X = x \mid Y = 1) &= f_1(x) & \Pr(X = x \mid Y = -1) &= f_2(x)
\end{align*}
\begin{remark}
    We will always write probabilities like this, but understand them to be
    densities whenever appropriate.
\end{remark}
From Bayes' rule, we have the \emph{posterior}
$\eta\colon \mathcal{X} \to [0, 1]$ defined by
\begin{align*}
    \eta(x) &\coloneq \Pr(Y = 1 \mid X = x) \\
         &= \frac{\Pr(X = x \mid Y = 1) \Pr(Y = 1)}{\Pr(X = x)} \\
         &= \frac{f_1(x) p_1}{f_1(x) p_1 + f_2(x) p_2}
\end{align*}
We can then define the \emph{Bayes classifier} as \begin{align*}
    h^*(x) &\coloneq \sgn(2\eta(x) - 1) \\
    &= \begin{cases}
        \hphantom{-}1 & \text{if } \eta(x) > \frac{1}{2}, \\
        -1 & \text{otherwise}
    \end{cases} \\
    &= \begin{cases}
        \hphantom{-}1 & \text{if } f_1(x) p_1 > f_2(x) p_2, \\
        -1 & \text{otherwise}
    \end{cases}
\end{align*}

For the specific case of multivariate Gaussians, \ie, $f_1$ and $f_2$ of the
form \begin{align*}
    N(x \mid \mu, \Sigma) &= \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}
        \exp\left(-\frac{1}{2} \norm{x - \mu}^2_{\Sigma^{-1}}\right)
\end{align*} with same covariance $\Sigma$ but different means $\mu_1$ and $\mu_2$,
we write \[
    h^*(x) = \begin{cases}
        \hphantom{-}1 & \text{if } \log \frac{\eta(x)}{1 - \eta(x)} > 0 \\
        -1 & \text{otherwise}
    \end{cases}
\] Now \begin{align*}
    \log \frac{\eta(x)}{1 - \eta(x)}
        &= \log \frac{f_1(x) p_1}{f_2(x) p_2} \\
        &= \log \frac{p_1}{p_2} - \frac12 \braket{x - \mu_1}{\Sigma^{-1}}{x - \mu_1}
            + \frac12 \braket{x - \mu_2}{\Sigma^{-1}}{x - \mu_2} \\
        &= \log \frac{p_1}{p_2} + \braket{\mu_1 - \mu_2}{\Sigma^{-1}}{x}
            - \frac12 \left(\norm{\mu_1}^2_{\Sigma^{-1}} - \norm{\mu_2}^2_{\Sigma^{-1}}\right) \\
        &= \innerp{w}{x} - b
\end{align*} where $w = \Sigma^{-1}(\mu_1 - \mu_2)$ (since $\Sigma$ is symmetric) and
$b$ is something.
Thus $h^*(x) = \sgn(\innerp{w}{x} - b)$.
\begin{remark}
    $\innerp{w}{x} = b$ is a hyperplane in $\R^d$, dividing the space into two
    half-spaces: $\innerp{w}{x} < b$ and $\innerp{w}{x} > b$.
    So a line is a very good guess for a classifier!
\end{remark}

\begin{exercise}
    Examine the special case of $\Sigma_1 = \sigma_1^2 I$ and
    $\Sigma_2 = \sigma_2^2 I$.
\end{exercise}
\begin{solution}
    We have \begin{equation*}
        \log \frac{f_1(x)}{f_2(x)}
            = d \log \frac{\sigma_2}{\sigma_1}
                - \frac{1}{2\sigma_1^2} \norm{x - \mu_1}^2
                + \frac{1}{2\sigma_2^2} \norm{x - \mu_2}^2
    \end{equation*}
    Thus we choose class $1$ when \[
        d \log \sigma_1 + \frac{1}{2\sigma_1^2}
            < d \log \sigma_2 + \frac{1}{2\sigma_2^2}
    \] and class $2$ otherwise.
\end{solution}

\section{How Good is the Bayes Classifier?} \label{sec:bayest}
We wish to compute the error $\Pr(h(X) \ne Y)$ for some rule $h$.
\begin{align*}
    \Pr(h(X) \ne Y)
        &= \E_{XY} \ind{h^*(X) \ne Y} \\
        &= \E_X \E_{Y \mid X} \ind{h^*(X) \ne Y} \\
    \intertext{but}
    \E_{Y \mid X = x} \ind{h^*(X) \ne Y}
        &= \begin{cases}
            1 - \eta(x) & \text{if } h(x) = 1 \\
            \eta(x) & \text{if } h(x) = -1
        \end{cases} \tag{$*$} \\
        &= \eta(x) \ind{h^*(x) = -1} + (1 - \eta(x)) \ind{h^*(x) = 1}
\end{align*}
It is clear from $(*)$ that whenever $\eta(x) > 1 - \eta(x)$, setting
$h(x) = 1$ minimizes the error, and whenever $\eta(x) < 1 - \eta(x)$,
setting $h(x) = -1$ minimizes the error.

More rigorously, upon comparing with $h^*$,
\begin{align*}
    \E_{Y \mid X} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y})
        &= \E_{Y \given X} (\ind{h^*(X) = Y} - \ind{h^*(X) \ne Y}) \\
        &= \eta(x) (\ind{h^*(X) = 1} - \ind{h^*(X) = -1}) \\
        &\qquad + (1 - \eta(x)) (\ind{h^*(X) = -1} - \ind{h^*(X) = 1}) \\
        &= (2 \eta(x) - 1)(\ind{h^*(X) = 1} - \ind{h^*(X) = -1}) \\
        &= (2 \eta(x) - 1)(2 \ind{h^*(X) = 1} - 1)
\end{align*}
The second term is $1$ when the first term is positive, and $-1$ when it is
negative.

Thus \begin{align*}
    \E_{XY} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y})
        &= \E_X \E_{Y \mid X} (\ind{h(X) \ne Y} - \ind{h^*(X) \ne Y}) \\
        &= \E_X \abs{2 \eta(x) - 1} \ge 0.
\end{align*}
This proves that the Bayes classifier is the classifier with the lowest
probability of error. \\
(This is theorem 2.1 in DGL.)
