\lecture{2024-02-09}{SVM: Wolfe dual and kernel trick}
Substituting \cref{eq:svm-w} into the Lagrangian gives \begin{align*}
    L &= \frac12 \norm*[\Big]{\sum_i \lambda_i y_i x^{(i)}}^2
        - \sum_i \lambda_i
            \innerp[\Big]{\sum_j \lambda_j y_j x^{(j)}}{y_i x^{(i)}}
        + \sum_{i=1}^{m} \lambda_i \\
      &= \sum_i \lambda_i - \frac12 \sum_{i,j}
            \lambda_i \lambda_j \innerp{y_i x^{(i)}}{y_j x^{(j)}}
\end{align*}
Thus using the Wolfe dual (\cref{sec:wolfe_dual}),
the SVM problem is to solve \[
    \quad\max_\lambda \sum_i \lambda_i - \frac12 \sum_{i,j}
        \lambda_i \lambda_j \innerp*{y_i x^{(i)}}{y_j x^{(j)}}
    \quad \text{subject to} \quad
    \lambda_i \ge 0
\] If we find such a $\lambda$, we have \begin{align*}
    w &= \sum_{i=1}^m \lambda_i y_i x^{(i)}
    \shortintertext{and the classifier}
    h(x) &= \sgn \innerp{w}{x}.
\end{align*}

Except\dots\ this is \textbf{NOT} the SVM problem.
The SVM problem does not absorb the constant $b$ into the vector $w$.
\setcounter{theorem}{0}
\begin{definition*}[The SVM problem] \label{def:svm}
    The \emph{support vector machine} (SVM) problem is to find the
    hyperplane with the largest margin.
    That is, find $w$ and $b$ that solve \[
        \max_{w, b} \min_i \frac{y_i (\innerp{w}{x^{(i)}} + b)}{\norm{w}}.
    \]
\end{definition*}
Notice that the norm in the denominator \emph{does not} include $b$.
Through much the same machinery as before, one arrives at the problem \[
    \boxed{
        \max_\lambda \sum_i \lambda_i - \frac12 \sum_{i,j}
            \lambda_i \lambda_j y_i y_j \innerp{x^{(i)}}{x^{(j)}}
        \quad \text{subject to} \quad
        \begin{cases}
            \lambda_i \ge 0, \\
            \sum_i \lambda_i y_i = 0.
        \end{cases}
    }
\] This determines $w$ as before: $w = \sum_i \lambda_i y_i x^{(i)}$.

Note that the only dependence on $x^{(i)}$ is through the inner product.
Thus we can use the \emph{kernel trick} to solve the SVM problem
in linearly non-separable cases.

Suppose $(x^{(i)}, y_i)$ are not linearly separable, but there is a
transformation $\Phi$ such that $(\Phi(x^{(i)}), y_i)$
are linearly separable.
Then we can apply SVM to the transformed dataset.
\[
    \max_{\lambda} \sum_i \lambda_i - \frac12 \sum_{i,j}
        \lambda_i \lambda_j \innerp*{y_i \Phi(x^{(i)})}{y_j \Phi(x^{(j)})}
    \quad \text{subject to} \quad
    \begin{cases}
        \lambda_i \ge 0, \\
        \sum_i \lambda_i y_i = 0.
    \end{cases}
\] If we can compute $\innerp*{\Phi(x^{(i)})}{\Phi(x^{(j)})}$,
then we can solve the SVM problem for the transformed dataset.
