\section{Generalization Errors} \label{sec:regression:generalization}
\lecture{2024-03-05}{Bias-variance decomposition with application to least squares}

Let $f(x; \mcD)$ be the classifier returned on the data set $\mcD$.
The generalization error is given by \begin{align*}
    \E_\mcD R(f(x; \mcD)) &= \E_\mcD \E_{X, Y} \ell(f(X; \mcD), Y) \\
    &= \E_{X, Y} \E_\mcD \ell(f(X; \mcD), Y)
\end{align*}
From \cref{eq:bias-variance} again, \begin{align*}
    \E_\mcD \ell(f(X; \mcD), Y)
        &= \E_\mcD (f(X; \mcD) - Y)^2 \\
        &= \Var_\mcD f(X; \mcD) + (\E_\mcD f(X; \mcD) - Y)^2
\end{align*}
So the generalization error is given by \begin{align*}
    \E_{X, Y} \Var_{\mcD \given X} f(X; \mcD)
        &+ \E_{X, Y} (\E_{\mcD \given X} f(X; \mcD) - Y)^2 \\
    &\quad= \E_X \Var_{\mcD \given X} f(X; \mcD)
        + \E_X \E_{Y \given X} (Y - \E_{\mcD \given X} f(X; \mcD))^2
    \intertext{and again using \cref{eq:bias-variance},}
    &\quad= \E_X \Var_{\mcD \given X} f(X; \mcD)
        + \E_X \Var_{Y \given X} Y
        + \E_X \Big(\E_{\mcD \given X} f(X; \mcD) - \E_{Y \given X} Y\Big)^2 \\
    &\quad= \underbrace{\E_X \Var_{\mcD \given X} f(X; \mcD)}_{\text{variance}}
        + \underbrace{\E_X \Var_{Y \given X} Y}_{\text{noise}}
        + \underbrace{\E_X \Big(\E_{\mcD \given X} f(X; \mcD) - f_B(X)\Big)^2}_{\text{bias}^2}
\end{align*}
For $Y = f^*(X) + \varepsilon$, this becomes \[
    \E_X \Var_{\mcD \given X} f(X; \mcD)
        + \sigma^2 + \E_X \Big(\E_{\mcD \given X} f(X; \mcD) - f^*(X)\Big)^2
\]

Finally, we come to the linear case, with specific algorithms.
\subsection{Least Squares} \label{sec:generalization:ls}
We have \[
    Y = \innerp w X + \varepsilon
\] and \[
    f(x; \mcD) = \innerp{w_{LS}} x
\] where \[
    w_{LS} = (D^\top D)^{-1} D^\top t
\] We have three terms to compute:
\begin{itemize}
    \item \textbf{Bias:} \[
        \E_{\mcD \given X} f(X; \mcD) - \innerp w X
    \]
    \item \textbf{Variance:} \[
        \Var_{\mcD \given X} f(X; \mcD)
    \]
    \item \textbf{Noise:} This we already know to be \[
        \E_X \Var_{Y \given X} Y = \sigma^2
    \]
\end{itemize}
Since $t = D w + \varepsilon$, $\E[t] = D w$.
So \[
    \E_{\mcD \given X} f(X; \mcD)
        = \E_{\mcD \given X} \innerp X {w_{LS}}
        = \innerp X {\E_{\mcD \given X} w_{LS}}
        = \innerp X {\E_{\mcD \mid X} {D^\top D}^{-1} D^\top D w}
        = \innerp X w.
\] Thus the bias is $0$.

The variance is hard to compute.
We will find an estimate using the \emph{fixed design setting}.
That is, we will assume that the data set $\mcD$ precisely represents the
distribution of $X$. \[
    \Pr(X = x) = \frac1n \sum_{i=1}^n [x = x^{(i)}].
\] Then \begin{align*}
    \Var_{\mcD \given X} f(X; \mcD)
    &= \E_{\mcD \given X} (f(X; \mcD) - \E_{\mcD \given X} f(X; \mcD))^2 \\
    &= \E_{\mcD \given X} (f(X; \mcD) - \innerp w X)^2 \\
    &= \E_{\mcD \given X} \innerp{w_{LS} - w} X^2 \\
    &= \E_\varepsilon \frac1n \sum_{i=1}^n \innerp{(D^\top D)^{-1} D^\top \varepsilon}
            {x^{(i)}}^2 \tag{fixed design} \\
    &= \frac1n \E_\varepsilon \varepsilon^\top D (D^\top D)^{-1} \sum_{i=1}^n x^{(i)} x^{(i)\top} (D^\top D)^{-1} D^\top \varepsilon \\
    &= \frac1n \E_\varepsilon \varepsilon^\top D (D^\top D)^{-1} (D^\top D) (D^\top D)^{-1} D^\top \varepsilon \tag{$\star$} \label{eq:x_cov} \\
    &= \frac1n \E_\varepsilon  \Tr\paren{\varepsilon^\top D (D^\top D)^{-1} D^\top \varepsilon}
        \tag{since it is a scalar} \\
    &= \frac1n \Tr((D^\top D)^{-1} D^\top \E_\varepsilon \varepsilon
            \varepsilon^\top D (D^\top D)^{-1}) \tag{cyclic property} \\
    &= \frac{\sigma^2}n \Tr((D^\top D)^{-1} D^\top D) \\
    &= \frac{\sigma^2 d}n
\end{align*}
In \cref{eq:x_cov}, we used the fact that \[
    \sum_{i=1}^n x^{(i)} x^{(i)\top} = D^\top D.
\] To see this, note that \[
    (D^\top D)_{ij} = \sum_{k=1}^n x^{(k)}_i x^{(k)}_j
    \quad\text{and}\quad
    (x^{(k)} x^{(k)\top})_{ij} = x^{(k)}_i x^{(k)}_j.
\]
Thus we have \[
    \boxed{R(f_{LS}) = \sigma^2 \paren{1 + \frac dn}.}
\]
