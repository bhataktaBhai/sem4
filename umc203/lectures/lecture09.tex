\chapter{Large margin classification} \label{chp:svm}
\lecture{2024-02-08}{Large margin classification}
Let $\mcD = \set{(x^{(i)}, y_i)}_{i=1}^m$ be a linearly separable dataset.
The perceptron algorithm finds a separating hyperplane,
but there are many such hyperplanes.
Which one is the best?

We can focus on the margin of the hyperplane.
The margin is as defined in \cref{def:margin}.
The hyperplane with the largest margin is deemed the best.

\begin{definition*}[The SVM problem] \label{def:svm-false}
    The \emph{support vector machine} (SVM) problem is to find the
    hyperplane with the largest margin.
    That is, find $w$ that solves \[
        \max_w \min_i \frac{y_i \innerp{w}{x^{(i)}}}{\norm{w}}.
    \]
\end{definition*}
What about the more general classifiers using $\innerp w x + b$?
We can append a constant $1$ to each $x^{(i)}$ and append $b$ to $w$.
Hence we can restrict our attention to the case where $b = 0$.

Note that the objective function is homogeneous in $w$.
So we can scale $w$ such that $\min_i y_i \innerp{w}{x^{(i)}} = 1$.
Then the problem becomes \[
    \max_w \frac1{\norm{w}} \quad \text{subject to} \quad
    \min_i y_i \innerp{w}{x^{(i)}} = 1.
\] When is $\min_i y_i \innerp{w}{x^{(i)}} = 1$?
When $\innerp{w}{y_i x^{(i)}} \ge 1$ for all $i$, but also
$\innerp{w}{y_i x^{(i)}} = 1$ for some $i$.
What if $\innerp{w}{y_i x^{(i)}} > 1$ for all $i$?
Then $w$ can be shrunken to increase the objective while still satisfying
the constraints.
Thus the problem becomes \[
    \max_w \frac1{\norm{w}} \quad \text{subject to} \quad
    \innerp{w}{y_i x^{(i)}} \ge 1 \text{ for all } i.
\] But maximizing $1/\norm{w}$ is the same as minimizing $\norm{w}^2$.
So we again rewrite the problem as \[
    \min_w \frac12 \norm{w}^2 \quad \text{subject to} \quad
    \innerp{w}{y_i x^{(i)}} \ge 1 \text{ for all } i.
\]

Note that $w \mapsto \norm{w}^2$ is a strictly convex function.
We have the Lagrangian \[
    L(w, \lambda) = \frac12 \norm{w}^2
        - \sum_{i=1}^m \lambda_i (\innerp{w}{y_i x^{(i)}} - 1)
\] and so the KKT conditions \begin{align}
    \nabla_w L(w, \lambda) = 0 \implies
        w &= \sum_{i=1}^m \lambda_i y_i x^{(i)} \label{eq:svm-w} \\
    \innerp{w}{y_i x^{(i)}} &\ge 1 \quad \text{for all } i, \\
    \lambda_i (\innerp{w}{y_i x^{(i)}} - 1) &= 0 \quad \text{for all } i.
\end{align}
If $\lambda_i > 0$, then $\innerp{w}{y_i x^{(i)}} = 1$.
If $\innerp{w}{y_i x^{(i)}} > 1$, then $\lambda_i = 0$.

The $x^{(i)}$s for which $\lambda_i > 0$ are called the
\emph{support vectors}.
These are at most the points for which $\innerp{w}{y_i x^{(i)}} = 1$.
