\chapter{Regression} \label{chp:regression}
\lecture[14]{2024-03-04}{Least squares and Ridge regression for linear models}
We move onto \emph{continuous} data.
Consider the data \begin{align*}
    \mcD &= \set{(x^{(i)}, y_i)}_{i=1}^n \subseteq \mcX \times \mcY \\
    \mcX &= \R^d \\
    \mcY &= \R \quad\longleftarrow \text{woah, continuous!}
\end{align*}
We again wish to find a classifier $f: \mcX \to \mcY$ that minimizes some
notion of loss.
We will first focus on the affine case, that is, $f(x) = \innerp w x + b$.
We can again employ the trick of appending a $1$ to the input vector,
so that we can focus on the linear case $f(x) = \innerp w x$.

\section{Least Squares Regression} \label{sec:regression:least_squares}
The most natural choice of loss function for continuous data is the squared
error loss, given by \[
    \ell(\hat{y}, y) = (\hat{y} - y)^2
\]
We can solve for the optimal $w$ by minimizing the empirical risk, that is,
\[
    w_{LS} = \argmin_{w \in \R^d}
            \frac12 \sum_{i=1}^n \paren{y_i - \innerp* w {x^{(i)}}}^2.
\] There are no constraints, so we need not worry about the KKT business.

\textbf{SOLVE!}

Define \begin{align*}
    D &= \begin{bmatrix}
        x^{(1)} & \cdots & x^{(n)}
    \end{bmatrix}^\top \in \R^{n \times d}
    & t &= \begin{bmatrix}
        y_1 \\ \vdots \\ y_n
    \end{bmatrix} \in \R^n
\end{align*}
Then the empirical risk can be written as \begin{align*}
    R(w) &= \frac12 \begin{bmatrix}
        \innerp{x^{(1)}} w - y_1 \\
        \vdots \\
        \innerp{x^{(n)}} w - y_n
    \end{bmatrix}^2 \\
    &= \frac12 \norm{Dw - t}^2
\end{align*}
Note that this is convex, since \begin{align*}
    \nabla R(w) &= D^\top (Dw - t) \\
    \nabla^2 R(w) &= D^\top D \succeq 0
\end{align*}
Thus the minimizer is given by \[
    \boxed{w_{LS} = (D^\top D)^{-1} D^\top t}
\]
What if $D^\top D$ is not invertible?
More realistically, what if $\det (D^\top D)$ is very small?
Adding a small multiple of the identity matrix to $D^\top D$ can help.

\section{Ridge Regression} \label{sec:regression:ridge}
Instead of minimizing the empirical risk, we can minimize the empirical risk
plus a regularization term.
\[
    w_{RR} = \argmin_{w \in \R^d}
                \frac12 \sum_i \paren{y_i - \innerp* w {x^{(i)}}}^2
                + \frac{\lambda}{2} \norm{w}^2
\] where $\lambda > 0$ is the regularization parameter.
This is called \emph{Ridge Regression}.

We now have \begin{align*}
    \nabla R(w) &= \frac12 D^\top (Dw - t) + \lambda w \\
    \nabla^2 R(w) &= D^\top D + \lambda I
\end{align*}
This is still convex, and now the minimizer is \[
    \boxed{w_{RR} = (D^\top D + \lambda I)^{-1} D^\top t}
\]

\section{Optimal Classifier} \label{sec:regression:optimal}
Suppose that $\mcD$ is drawn from a distribution with \[
    Y = f^*(X) + \varepsilon,
\] where $\varepsilon$ is a random variable with mean $0$ and variance
$\sigma^2$.
That is, \[
    \Pr(Y \le y \given X = x) = \Pr(f^*(x) + \varepsilon \le y).
\] If $\varepsilon$ is Gaussian, then \[
    \Pr(Y \given X = x) = N(f^*(x), \sigma^2)
\]

Let $f$ be a classifier.
Then the risk of $f$ is given by \begin{align*}
    R(f) &= \E_{X, Y} \ell(f(X), Y) \\
    &= \E_{X, Y} (f(X) - Y)^2 \\
    &= \E_X \E_{Y \given X} (Y - f(X))^2 \\
    &= \E_X \Var_{Y \given X} Y + \E_X (\E_{Y \given X} Y - f(X))^2 \\
    &\ge \E_X \Var_{Y \given X} Y
\end{align*}
The equality holds iff $f(X) = \E_{Y \given X} Y$ almost surely.
Thus the optimal classifier is given by \[
    f_B(x) = \E_{Y \given X} Y
\]

This computation is due to the following general result: \begin{align}
    \E (X - a)^2 &= \E (X - \E X + \E X - a)^2 \nonumber \\
    &= \E (X - \E X)^2 + \E [2(X - \E X)(\E X - a)] + \E (\E X - a)^2 \nonumber \\
    &= \Var X + (\E X - a)^2 \label{eq:bias-variance}
\end{align}
This is called the \emph{bias-variance decomposition}.

For $Y = f^*(X) + \varepsilon$, we have \begin{align*}
    \E_X \Var_{Y \given X} Y &= f^*(X) & \Var_{Y \given X} Y &= \sigma^2
\end{align*} so that \[
    R(f) = \sigma^2 + \E_X (f(X) - f^*(X))^2 \ge \sigma^2
\] The optimal classifier is, unsurprisingly, $f^*$.
